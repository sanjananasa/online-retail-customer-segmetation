{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GF8Ens_Soomf",
        "lQ7QKXXCp7Bj",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjananasa/online-retail-customer-segmetation/blob/main/Unsupervised_machine_learning_capstone_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Online Retail Customer Segmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - UNSUPERVISED MACHINE LEARNING\n",
        "##### **Contribution**    - Individual\n",
        "\n",
        "####**Name**- Sanjana Nasa\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The aim of this machine learning project is to perform customer segmentation for an online retail business. Customer segmentation involves dividing customer base into distinct groups based on shared characteristics , behaviour or preferences. By effectively segmenting customers , business can gain valuable insights and tailor their marketing strategies to specific customer group , leading to improved  customer satisfaction and increased profitability.\n",
        "\n",
        "####The project utilises a dataset containing relevant information about the online reatil customers. The dataset includes features such as customer demographics , purchase history ,frequency of purchases , monetary value of purchases, and other relevant variables that can help in segmenting customers effectively.\n",
        "\n",
        "####The primary objective of this project is to apply machine learning techniques to segment the online retail customers into distinct groups based on theit purchasing behaviour and characteristics. This segmentation will help the business to better understand the customer base , identify patterns and trends and develop personalised marketing campaigns  to target each segment."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####In this project, your task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Variables Description**"
      ],
      "metadata": {
        "id": "lxxd6MER9AWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attribute Information:\n",
        "**InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "\n",
        "**StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "\n",
        "**Description:** Product (item) name.\n",
        "\n",
        "**Quantity:** The quantities of each product (item) per transaction. Numeric.\n",
        "\n",
        "**InvoiceDate**: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "\n",
        "**UnitPrice:** Unit price. Numeric, Product price per unit.\n",
        "\n",
        "**CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "\n",
        "**Country:** Country name. Nominal, the name of the country where each customer resides."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "blLgMY5ah45W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/Online Retail- Online Retail.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "#first 5 rows of data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#last 5 rows of data\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "n7iomivdicdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of rows=\",data.shape[0])\n",
        "print(\"number of columns=\",data.shape[1])"
      ],
      "metadata": {
        "id": "CCUtJq8-iqRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "miss_val=data.isnull().sum().sort_values(ascending=False)\n",
        "print(miss_val)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.matrix(data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The dataset has **541909** (five lakh forty one thousand nine hundred nine) rows and **8** columns.\n",
        "2.There are 5268 duplicate values.\n",
        "3.Two columns namely, **Description** and **customerID** has **1454**(one thousand four hundren fifty four) and **135080**(one lakh thirty five thousand and eighty) missing/null values respectively.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe([0.75,0.95,0.99])"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**When looking at the summary statistics generated by the describe function, it is apparent that some negative values exist in the data. (as we can see the min value for price and quantity is negative) , so need to explore these columns.**\n",
        "\n",
        "###**Another noteworthy observation is that the 99th percentile for both the UnitPrice and Quantity columns is low, while the maximum value is much higher. This suggests that there are outliers in the data, which could be due to the occasional purchase of valuable items. Additionally, the UnitPrice and Quantity columns are inversely related to each other.**"
      ],
      "metadata": {
        "id": "s2z692g4F2Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data.nunique().sort_values()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#converting data type of \"InvoiceDate\" from object to datetime\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking for negative values in \"Quantity\"  column\n",
        "data[data['Quantity']<0]"
      ],
      "metadata": {
        "id": "pkwiy60uCLw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given that, if the \"InvoiceNo\" starts with 'C', that indicates cancellation. And we can see above that negative values of \"Quantity\" corresponds to invoice no beginning with 'C'. So removing all those rows.**"
      ],
      "metadata": {
        "id": "pQVuoKkcYzi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing cancelled orders\n",
        "data=data[~data['InvoiceNo'].str.contains('C')]"
      ],
      "metadata": {
        "id": "kxzveLL9Y-Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we have seen above that we have null values present in our dataset like in CustomerID and Description.we can drop thode null values in customerID columns as we are making customer segmentation and keeping those null values make no sense**"
      ],
      "metadata": {
        "id": "ISKJytvnbDA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping null values\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "VNc4CIU5a1Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of data after dropping entries\n",
        "data.shape"
      ],
      "metadata": {
        "id": "77C_aTD0bSWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's check if there are still any null/missing values**"
      ],
      "metadata": {
        "id": "s_a7RqbbcfuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for null values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "3_HQTJGVcdKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no null values in the data now.**"
      ],
      "metadata": {
        "id": "DA0Nr6ancr26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets check data summary again\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "UC3F9WGdETSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have removed all the cancelled orders , we can see that there is no negative value in the data. But the minimum UnitPrice is seen to be 0 (zero) which is not possible.**"
      ],
      "metadata": {
        "id": "XE05IQGCEbeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how many values are present for unitprice==0\n",
        "\n",
        "len(data[data['UnitPrice']==0])"
      ],
      "metadata": {
        "id": "Z2Ej-RxZEbF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**almost 40 values are present where UnitPrice=0 . so will drop this values**"
      ],
      "metadata": {
        "id": "_g7Rn7C9FF33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking unitprice values greater than 0.\n",
        "data=data[data['UnitPrice']>0]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Ok9go-0JFfv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking shape of data after dropping certain values\n",
        "data.shape"
      ],
      "metadata": {
        "id": "eDB4YK3PFud3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Converted data type of \"InvoiceDate\" from object to datetime**\n",
        "\n",
        "**2. Here I dropped some InvoiceNo which starts with 'c' because 'c' indicates a cancellation.**\n",
        "\n",
        "**3. I dropped null values in customerID columns as we are making customer segmentation and keeping those null values make no sense.**\n",
        "\n",
        "**4. dropped rows where unit price was zero.**\n",
        "\n",
        "**5. After removing above entries, now we have 397884 rows and 8 columns.**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**"
      ],
      "metadata": {
        "id": "GqWbILIbIpuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting InvoiceDate to datetime. InvoiceDate is in format of 01-12-2010 08:26.\n",
        "data[\"InvoiceDate\"] = pd.to_datetime(data[\"InvoiceDate\"], format=\"%d-%m-%Y %H:%M\")"
      ],
      "metadata": {
        "id": "CHWZ-0XhS1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"year\"] = data[\"InvoiceDate\"].apply(lambda x: x.year)\n",
        "data[\"month_num\"] = data[\"InvoiceDate\"].apply(lambda x: x.month)\n",
        "data[\"day_num\"] = data[\"InvoiceDate\"].apply(lambda x: x.day)\n",
        "data[\"hour\"] = data[\"InvoiceDate\"].apply(lambda x: x.hour)\n",
        "data[\"minute\"] = data[\"InvoiceDate\"].apply(lambda x: x.minute)"
      ],
      "metadata": {
        "id": "pL5G2v_hTq0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting month from the Invoice date\n",
        "data['Month']=data['InvoiceDate'].dt.month_name()"
      ],
      "metadata": {
        "id": "dGgtbOvIUJBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting day from the Invoice date\n",
        "data['Day']=data['InvoiceDate'].dt.day_name()"
      ],
      "metadata": {
        "id": "23UeRp6yUmz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['TotalAmount']=data['Quantity']*data['UnitPrice']"
      ],
      "metadata": {
        "id": "iqiEcUoSUtQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets have a look on dataframe after adding new columns\n",
        "data.head()"
      ],
      "metadata": {
        "id": "hAc2HkxYU6Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-1** (**Popularity of product type**)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#top five most popular products among customers\n",
        "\n",
        "Description_df=data['Description'].value_counts().reset_index()\n",
        "Description_df.rename(columns={'index': 'Description_Name'}, inplace=True)\n",
        "Description_df.rename(columns={'Description': 'Count'}, inplace=True)\n",
        "#top 5 Description Name\n",
        "Description_df.head()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Top 5 Product Name')\n",
        "sns.barplot(x='Count',y='Description_Name',data=Description_df[:5], palette='spring_r');\n"
      ],
      "metadata": {
        "id": "oeBTZzJpjBAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bottom 5 products (least popular among customers)\n",
        "Description_df.tail()"
      ],
      "metadata": {
        "id": "AH4af3PIjiXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualisation code\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Bottom 5 Product Name')\n",
        "sns.barplot(x='Count',y='Description_Name',data=Description_df[-5:], palette='spring_r');\n"
      ],
      "metadata": {
        "id": "zNh4-pXUmhRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I chose bar chart to compare the counts of different products.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Top 5 most selling products are:**\n",
        "1. WHITE HANGING HEART T-LIGHT HOLDER\n",
        "\n",
        "2.\tREGENCY CAKESTAND 3 TIER\n",
        "\n",
        "3.\tJUMBO BAG RED RETROSPOT\n",
        "\n",
        "4.\tASSORTED COLOUR BIRD ORNAMENT\n",
        "\n",
        "5.\tPARTY BUNTING\n",
        "\n",
        "###**5 least selling products are:**\n",
        "1. RUBY GLASS CLUSTER EARRING\n",
        "\n",
        "2. PINK CHRYSANTHEMUMS ART FLOWER\n",
        "\n",
        "3. 72 CAKE CASES VINTAGE CHRISTMAS\n",
        "\n",
        "4. WALL ART , THE MAGIC FOREST\n",
        "\n",
        "5. PAPER CRAFT , LITTLE BIRDIE"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When you know your best selling and least selling items, you can spend more time in promoting your best sellers and can look for ways to improve poor performing products.**"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 2** **(Best sellers StockCode wise)**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 products stock code wise\n",
        "StockCode_df=data['StockCode'].value_counts().reset_index()\n",
        "StockCode_df.rename(columns={'index': 'StockCode_Name'}, inplace=True)\n",
        "StockCode_df.rename(columns={'StockCode': 'Count'}, inplace=True)\n",
        "#top 5 stockcode name\n",
        "StockCode_df.head()"
      ],
      "metadata": {
        "id": "OW4VDDRJVLAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#plot top 5 stockcode name\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Top 5 Stock Name')\n",
        "sns.barplot(x='Count',y='StockCode_Name',data=StockCode_df[:5], palette='spring_r')\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare the selling counts of product as per their stock code.**"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Product with stock code 85123A is the product with highest sales.**"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart - 3** **(number of customers from different countries)**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#country wise number of customers\n",
        "cust_count=data['Country'].value_counts()\n",
        "print(cust_count)"
      ],
      "metadata": {
        "id": "7WOuNAl5eT1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "cust_count.head().plot(kind='bar')\n",
        "plt.title(\"top five countries the customers belong to\")\n",
        "plt.xlabel(\"Countries\")\n",
        "plt.ylabel(\"counts\")"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I have chosen bar chart to compare the count of customers from different countries.**"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can clearly see that maximum number of customers are from United Kingdom, which makes sense as the company itself is UK-based. After that we have Germany , France , EIRE (Ireland) which have almost equal number of customers and last is  Spain.**"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chart - 4**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "cust_count.tail().plot(kind='bar')\n",
        "plt.title(\"Countries with least number of customers\")\n",
        "plt.xlabel(\"Countries\")\n",
        "plt.ylabel(\"count\")"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Lithuania , Brazil , Czech Republic , Bahrain and Saudi Arabia has the  lowest customer base.**"
      ],
      "metadata": {
        "id": "bEJsuQLAMdEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 6** **(Distribution of unit price)**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "#distribution of unit price\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('UnitPrice distribution')\n",
        "sns.distplot(data['UnitPrice'])"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used distplot to show the variation in the distribution of data points of \"UnitPrice\"**"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the distribution of unit price, we can say that most items have a lower price range.**"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lower prices mean lower profit margins, so they'll have to sell a higher volume in order to survive in the market.**"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 5** **(Distribution of quantity)**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "#distribution of Quantity\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('distribution of Quantity')\n",
        "sns.distplot(data['Quantity'],color=\"r\")"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used distplot to show the variation in the distribution of data points of \"Quantity\"**"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we can see that its a Positively skewed (or right-skewed) distribution. It is a type of distribution in which most values are clustered around the left tail of the distribution.This means that the most extreme values are on the right side.**"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 7 (Top 10 customers as per customerID)**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#top 10 customers in terms of purchasing counts\n",
        "top_10_customers=data['CustomerID'].value_counts().reset_index().rename(columns={'index':'CustomerID','CustomerID':'Product_purchasing_count'}).head(10)\n",
        "print(top_10_customers)"
      ],
      "metadata": {
        "id": "lI6k5TovhWBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=top_10_customers['CustomerID'],y=top_10_customers['Product_purchasing_count'].head(10),palette='spring_r')\n",
        "plt.title('Top 10 frequent Customers.')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare the number of purchases done by different customers.**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CustomerID- 17841 had purchased highest number of products**\n",
        "\n",
        "**CustomerID-14911 is the 2nd higest customer who purchased the most the products**"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart - 8 (Monthwise sales count)**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sales count in different months\n",
        "sales_in_month=data['Month'].value_counts().reset_index().rename(columns={'index':'Month','Month':'Sales_count'})\n",
        "sales_in_month"
      ],
      "metadata": {
        "id": "LO7TpgLVnY2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Sales count in different months.\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(x=sales_in_month['Month'],y=sales_in_month['Sales_count'],palette='spring_r')\n",
        "plt.title('Sales count in different Months ')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare the monthly sales count.**"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most of the sale happened in Novmenber month.**\n",
        "\n",
        "**February Month had least sales.**"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chart-9 (sales count day wise)**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sales count on different days of th week\n",
        "sales_on_day_basis=data['Day'].value_counts().reset_index().rename(columns={'index':'Day',\"Day\":'Sale_count'})\n",
        "sales_on_day_basis"
      ],
      "metadata": {
        "id": "AoIxHgIVsjEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Sales count on different days.\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(x=sales_on_day_basis['Day'],y=sales_on_day_basis['Sale_count'],palette='spring_r')\n",
        "plt.title('Sales count on different Days ')"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare sale count on different days of week.**"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can clearly see the sales on Thursdays being the highest and least on Fridays.**"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chart - 10 (sales time in different day times)**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check for unique values in hour column\n",
        "data['hour'].unique()"
      ],
      "metadata": {
        "id": "6uj0jcyEAKZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a function to categorise the purchase time as morning , afternoon or evening\n",
        "def time(time):\n",
        "  if (time==6 or time==7 or time==8 or time==9 or time==10 or time==11) :\n",
        "    return'Morning'\n",
        "  elif (time==12 or time==13 or time==14 or time==15 or time==16 or time==17):\n",
        "    return 'Afternoon'\n",
        "  else:\n",
        "    return 'Evening'"
      ],
      "metadata": {
        "id": "KAeT0NYFAZFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Day_time_type']=data['hour'].apply(time)"
      ],
      "metadata": {
        "id": "KZAmcj5vA52-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of sales happened during different time of the day(morning, evening or afternoon)\n",
        "sales_timing=data['Day_time_type'].value_counts().reset_index().rename(columns={'index':'Day_time_type','Day_time_type':'Sales_count'})\n",
        "sales_timing"
      ],
      "metadata": {
        "id": "3hSy_BmaBE2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Sales count on different days.\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=sales_timing['Day_time_type'],y=sales_timing['Sales_count'],palette='spring_r')\n",
        "plt.title('Sales count in different day timings')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare the sale count with respect to different times of day.**"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most of the sales happened in the afternoon and least in the evening.**"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chart - 11 (Average amount spent by each customer)**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#average amount spent by customers\n",
        "average_amt=data.groupby('CustomerID')['TotalAmount'].mean().reset_index().rename(columns={'TotalAmount':'Average_amount_spent'}).sort_values('Average_amount_spent',ascending=False)\n",
        "average_amt"
      ],
      "metadata": {
        "id": "t277eMkUjIFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=average_amt['CustomerID'].head(5),y=average_amt['Average_amount_spent'].head(5),palette='spring_r')\n",
        "plt.title('Average amount spent by top 5 Customers')\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Used bar chart to compare the average amount spent by customers.**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**77183 (Dollars)is the highest average amount spent by the CustomerID-12346**\n",
        "\n",
        "**56157 (Dollars) is the 2nd highest average amount spent by the CustomerID-16446**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MODEL BUILDING**"
      ],
      "metadata": {
        "id": "aWYx8KCQyHSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**RFM Model Analysis**"
      ],
      "metadata": {
        "id": "e0LtrbyAyXym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**What is RFM?**\n",
        "\n",
        "RFM**(Recency, Frequency, Monetary)** analysis is a widely used customer segmentation technique in marketing and analytics. It helps businesses understand and categorize their customers based on three key factors:\n",
        "\n",
        "\n",
        "1.How recently they made a purchase **(Recency)**,\n",
        "\n",
        "2.How frequently they make purchases **(Frequency),**\n",
        "\n",
        "3.How much they spend **(Monetary value)**.\n",
        "\n",
        "\n",
        "RFM analysis enables businesses to identify and target different customer segments with customized marketing approaches.\n",
        "\n",
        "\n",
        "###**Why it is Needed?**\n",
        "\n",
        "RFM Analysis is a marketing framework that is used to understand and analyze customer behaviour based on the above three factors RECENCY, Frequency, and Monetary.\n",
        "\n",
        "\n",
        "The RFM Analysis will help the businesses to segment their customer base into different homogenous groups so that they can engage with each group with different targeted marketing strategies."
      ],
      "metadata": {
        "id": "tYppdALCygQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy of original dataframe\n",
        "rfm_df=data.copy()"
      ],
      "metadata": {
        "id": "u6FrjMTeyTkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "xz1OrX3j1f9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Recency = Latest Date - Last Inovice Data,\n",
        " Frequency = count of invoice no. of transaction(s),\n",
        "  Monetary = Sum of Total '''\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "#Set Latest date 2011-12-10 as last invoice date was 2011-12-09. This is to calculate the number of days from recent purchase\n",
        "Latest_Date = dt.datetime(2011,12,10)\n",
        "\n",
        "#Create RFM Modelling scores for each customer\n",
        "rfm_df = rfm_df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (Latest_Date - x.max()).days, 'InvoiceNo': lambda x: len(x), 'TotalAmount': lambda x: x.sum()})\n",
        "\n",
        "#Convert Invoice Date into type int\n",
        "rfm_df['InvoiceDate'] = rfm_df['InvoiceDate'].astype(int)\n",
        "\n",
        "#Rename column names to Recency, Frequency and Monetary\n",
        "rfm_df.rename(columns={'InvoiceDate': 'Recency',\n",
        "                         'InvoiceNo': 'Frequency',\n",
        "                         'TotalAmount': 'Monetary'}, inplace=True)\n",
        "\n",
        "rfm_df.reset_index().head()"
      ],
      "metadata": {
        "id": "DTRlGETMqGA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Stats= Recency\n",
        "rfm_df.Recency.describe()"
      ],
      "metadata": {
        "id": "_UA6tDwjqqBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=rfm_df['Recency'])\n",
        "plt.title('Distribution of Recency')"
      ],
      "metadata": {
        "id": "LKP6PyD3qwnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**We  can clearly observe from above plot that distribution of  'Recency'  is positively skewed.**"
      ],
      "metadata": {
        "id": "rCgaQBbNq4tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Stats= Frequency\n",
        "rfm_df.Frequency.describe()"
      ],
      "metadata": {
        "id": "qRhActDUrYJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=rfm_df['Frequency'])\n",
        "plt.title('Distribution of Frequency')"
      ],
      "metadata": {
        "id": "RqlXgpBDr8jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of Frequency is highly right skewed.**"
      ],
      "metadata": {
        "id": "HmAVFjSPsJfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Stats= Monetary\n",
        "rfm_df['Monetary'].describe()"
      ],
      "metadata": {
        "id": "itYYn6zfsavb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=rfm_df['Monetary'])\n",
        "plt.title('Distribution of Monetary')"
      ],
      "metadata": {
        "id": "P6BfFYELsoyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of Monetary is highly right skewed.**"
      ],
      "metadata": {
        "id": "d-lzS205sxpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into four segment using Quantile\n",
        "quantile = rfm_df.quantile(q = [0.25,0.50,0.75])\n",
        "\n",
        "#Converting quantiles to a dictionary, that would be easier to use.\n",
        "quantile = quantile.to_dict()"
      ],
      "metadata": {
        "id": "h-U3yprGvd66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantile"
      ],
      "metadata": {
        "id": "gm2NwnuivgWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to create R, F and M segments\n",
        "# arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\n",
        "# lower the recency, good for the company\n",
        "\n",
        "def RScoring(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 1\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 2\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "\n",
        "        # arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\n",
        "        # higher value of frequency and monetary lead to a good consumer. Here higher value = 1 in reverse way.\n",
        "\n",
        "def FnMScoring(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 4\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 3\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "GiYS1mmFv2wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating and adding R,F and M segments values columns in the existing dataset to show R,F,M segment values\n",
        "rfm_df[\"R\"] = rfm_df['Recency'].apply(RScoring,args=('Recency',quantile,))\n",
        "rfm_df[\"F\"] = rfm_df['Frequency'].apply(FnMScoring,args=('Frequency',quantile,))\n",
        "rfm_df[\"M\"] = rfm_df['Monetary'].apply(FnMScoring,args=('Monetary',quantile,))\n",
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "s0GWTt36yPax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column to combine RFM score\n",
        "rfm_df['RFM_Group'] = rfm_df.R.map(str)+rfm_df.F.map(str)+rfm_df.M.map(str)\n",
        "\n",
        "#Calculate and Add RFMScore value column showing total sum of RFMGroup values\n",
        "rfm_df['RFM_Score'] = rfm_df[['R', 'F', 'M']].sum(axis = 1)\n",
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "wgElOTQHyfaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df.info()"
      ],
      "metadata": {
        "id": "5FyT4Bhyyt_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df['RFM_Score'].unique()"
      ],
      "metadata": {
        "id": "CtT4Je-FzUA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign Loyalty Level to each customer\n",
        "Loyalty_Level = ['Platinaum','Gold','Silver','Bronz']\n",
        "\n",
        "Score_cut = pd.qcut(rfm_df['RFM_Score'],q = 4,labels=Loyalty_Level)\n",
        "rfm_df['RFM_Loyalty_Level'] = Score_cut.values\n",
        "rfm_df.reset_index().head()"
      ],
      "metadata": {
        "id": "zxNzNHjMzZuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the data For RFM group = 111\n",
        "rfm_df[rfm_df['RFM_Group'] == '111'].sort_values(\"Monetary\",ascending = False).reset_index().head(10)"
      ],
      "metadata": {
        "id": "1ChqYEs2z1w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loyalty level\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(rfm_df['RFM_Loyalty_Level'],palette='spring_r')\n",
        "plt.title('Loyalty Level of Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_6Ffnq60MUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_based_on_RFM = rfm_df[['Recency','Frequency','Monetary','RFM_Loyalty_Level']]\n",
        "\n",
        "segmentation_based_on_RFM.groupby('RFM_Loyalty_Level').agg({\n",
        "    'Recency': ['mean', 'min', 'max'],\n",
        "    'Frequency': ['mean', 'min', 'max'],\n",
        "    'Monetary': ['mean', 'min', 'max','count']\n",
        "})"
      ],
      "metadata": {
        "id": "BpVaeXlb59TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle negative and zero values so as to handle infinite numbers during log transformation\n",
        "def handle_neg_n_zero(num):\n",
        "    if num <= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return num\n",
        "#Apply handle_neg_n_zero function to Recency and Monetary columns\n",
        "rfm_df['Recency'] = [handle_neg_n_zero(x) for x in rfm_df.Recency]\n",
        "rfm_df['Monetary'] = [handle_neg_n_zero(x) for x in rfm_df.Monetary]"
      ],
      "metadata": {
        "id": "jsFavZYY98ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform Log transformation to bring data into normal or near normal distribution\n",
        "Log_rfm_df = rfm_df[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis = 1).round(3)"
      ],
      "metadata": {
        "id": "jkIiJGhX-Ifp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Now let's Visualize the Distribution of Recency,Frequency and Monetary.**"
      ],
      "metadata": {
        "id": "qV3kwlrY_QxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of Recency\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=Log_rfm_df['Recency'])\n",
        "plt.title('Distribution of Recency')"
      ],
      "metadata": {
        "id": "0etCqVpc-P84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of Frequency\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=Log_rfm_df['Frequency'])\n",
        "plt.title('Distribution of Frequency')"
      ],
      "metadata": {
        "id": "vk-93eDM-jwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of Monetary\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(x=Log_rfm_df['Monetary'])\n",
        "plt.title('Distribution of Monetary')"
      ],
      "metadata": {
        "id": "9_M_04Hr_Lxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df['Recency_log'] = rfm_df['Recency'].apply(math.log)\n",
        "rfm_df['Frequency_log'] = rfm_df['Frequency'].apply(math.log)\n",
        "rfm_df['Monetary_log'] = rfm_df['Monetary'].apply(math.log)"
      ],
      "metadata": {
        "id": "zND2-HVtAJws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df"
      ],
      "metadata": {
        "id": "YdcZgtXyAVUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 1**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**K-Means Clustering**"
      ],
      "metadata": {
        "id": "-4RvuxTAA3Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "vtfZDdevbTbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yellowbrick"
      ],
      "metadata": {
        "id": "q8z4CnkkbWV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Before implementing the Kmeans Clustering algorithm we need to decide the number of clusters in algorithm as input. So we will be finding the minimum number of clusters required by using Elbow method.**"
      ],
      "metadata": {
        "id": "8qrv0jlbbizI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1) Applying Elbow Method on Recency and Monetary.**\n",
        "\n",
        "The elbow method is a heuristic used to determine the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use."
      ],
      "metadata": {
        "id": "xBu-JmRMb6vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking Recency and Monetory_log in list.\n",
        "Recency_and_Monetary_feat=['Recency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Recency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#applying Elbow Method\n",
        "wcss = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    wcss[k] = km.inertia_\n",
        "\n",
        "\n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJHxPn1Kb9rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Here we can see that Optimal value for cluster came out to be 2.**"
      ],
      "metadata": {
        "id": "IFYqxrEOcpWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Silhouette Score *(Validating Above optimal cluster value(i.e optimal_cluster=2)***\n",
        "\n",
        "Silhouette Score is a metric to evaluate the performance of clustering algorithm. It uses compactness of individual clusters(intra cluster distance) and separation amongst clusters (inter cluster distance) to measure an overall representative score of how well our clustering algorithm has performed."
      ],
      "metadata": {
        "id": "hwBjZ3YxdYF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking Recency and Monetory_log in list.\n",
        "Recency_and_Monetary_feat=['Recency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Recency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#Silhouette Score\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters,random_state=1)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "xPWf8ez9dXvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**The silhouette ranges from 1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate.**"
      ],
      "metadata": {
        "id": "U36K2qpFesVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Here we can see that for n_cluster=2 silhouette score is good as compared to others.(if values is close to 1 means data points are clustered very well to respective clusters and distance of that datapoint is very far from the other cluster).**"
      ],
      "metadata": {
        "id": "m6tmKJlldz2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xpHJ5zwvlmWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**We got good Silhouette plot for Cluster-2 but still few datapoints are on the negative side of the Silhouette Coefficient value  but its better than others.**\n",
        "\n",
        "##**So giving n_clusters=2 to K-means Model**"
      ],
      "metadata": {
        "id": "hBYzFGYomlgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying Kmeans_clustering algorithm\n",
        "kmeans_rec_mon = KMeans(n_clusters=2)\n",
        "kmeans_rec_mon.fit(X)\n",
        "y_kmeans= kmeans_rec_mon.predict(X)"
      ],
      "metadata": {
        "id": "FB1_zryJm-rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the clusters for the observation given in the dataset\n",
        "rfm_df['Cluster_based_rec_mon'] = kmeans_rec_mon.labels_\n",
        "rfm_df.head(10)"
      ],
      "metadata": {
        "id": "HjQopSzSnHG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Centers of the clusters(coordinates)\n",
        "centers = kmeans_rec_mon.cluster_centers_\n",
        "centers"
      ],
      "metadata": {
        "id": "jXDRjcEgnYKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting visualizing the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Recency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='winter')\n",
        "\n",
        "centers = kmeans_rec_mon.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8)"
      ],
      "metadata": {
        "id": "L5_6uvRUncd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Applying Elbow Method on Frequency and Monetary.**"
      ],
      "metadata": {
        "id": "fAcuv72WDPKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking frequency and Monetory_log in list.\n",
        "Frequency_and_Monetary_feat=['Frequency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Frequency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#applying Elbow Method\n",
        "wcss = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    wcss[k] = km.inertia_\n",
        "\n",
        "\n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZ4tUI7MDovR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Here we can see that Optimal value for cluster came out to be 2.**"
      ],
      "metadata": {
        "id": "HFaS1pSDE2w_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Silhouette Score (Validating Above optimal cluster value(i.e optimal_cluster=2)**"
      ],
      "metadata": {
        "id": "SpQW4jOWE5rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking frequency and Monetory_log in list.\n",
        "Frequency_and_Monetary_feat=['Frequency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Frequency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#Silhouette Score\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters,random_state=1)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "aXbrgqQlFJVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Here we can see the for n_cluster=2 silhouette score is good as compared to others.(if values is close to 1 means data points are clustered very well to respective clusters and distance of that datapoint is very far from the other cluster.)**"
      ],
      "metadata": {
        "id": "mdzfM8ZcFuCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_n_clusters = [2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5s8HMl93GPll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Silhouette Plot for Frequency and Monetary with cluster=2 is very good as compared to Recency and Monetary's Silhouette plot.**\n",
        "\n",
        "###**No datapoints are on the negative side of the Silhouette Coefficent values**"
      ],
      "metadata": {
        "id": "GdS-cHHaIqp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**So giving n_clusters=2 on Kmeans Model.**"
      ],
      "metadata": {
        "id": "dMQ3zgR7I5yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying Kmeans_clustering algorithm\n",
        "kmeans_freq_mon = KMeans(n_clusters=2)\n",
        "kmeans_freq_mon.fit(X)\n",
        "y_kmeans= kmeans_freq_mon.predict(X)"
      ],
      "metadata": {
        "id": "ZbFa_BcvIqLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the clusters for the observation given in the dataset\n",
        "rfm_df['Cluster_based_on_freq_mon'] = kmeans_freq_mon.labels_\n",
        "rfm_df.head(10)"
      ],
      "metadata": {
        "id": "Dso3uYEoJCmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Centers of the clusters(coordinates)\n",
        "centers = kmeans_freq_mon.cluster_centers_\n",
        "centers"
      ],
      "metadata": {
        "id": "7pw3WV-NMQGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting visualizing the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Frequency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='winter')\n",
        "\n",
        "centers = kmeans_freq_mon.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8)"
      ],
      "metadata": {
        "id": "iXZ_7F1tMWDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Applying Elbow Method on Recency, Frequency and Monetary.**"
      ],
      "metadata": {
        "id": "rCorCSyCMjxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking Recency_log, Frequency_log and Monetory_log in list.\n",
        "Recency_Frequency_and_Monetary_feat=['Recency_log','Frequency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Recency_Frequency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#applying Elbow Method\n",
        "wcss = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    wcss[k] = km.inertia_\n",
        "\n",
        "\n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QkjZYRCnMjXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Silhouette Score (Validating Above optimal cluster value(i.e optimal_cluster=2)**"
      ],
      "metadata": {
        "id": "NiK1jU3WV8Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taking Recency_log,Frequency_log and Monetory_log in list.\n",
        "Recency_Frequency_and_Monetary_feat=['Recency_log','Frequency_log','Monetary_log']\n",
        "\n",
        "# taking only values of recency and monetory in X.\n",
        "X=rfm_df[Recency_Frequency_and_Monetary_feat].values\n",
        "\n",
        "# standardising the data\n",
        "scaler=StandardScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n",
        "#Silhouette Score\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters,random_state=1)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "QK-qIBKIZ-D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Here we can see the for n_cluster=2 silhouette score is good as compared to others.(if values is close to 1 means data points are clustered very well to respective clusters and distance of that datapoint is very far from the other cluster.)**"
      ],
      "metadata": {
        "id": "hhm1JQLYaUXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8wEGHdhFaX20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Silhouette Plot for Frequency and Monetary with cluster=2 is good.**\n",
        "\n",
        "###**Still Few datapoints are on the negative side of the Silhouette Coefficent values(see below image). Still we can consider the clusters**"
      ],
      "metadata": {
        "id": "iWV_fID_mSg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**So giving n_clusters=2 on Kmeans Model.**"
      ],
      "metadata": {
        "id": "nEBidmEJme7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying Kmeans_clustering algorithm\n",
        "kmeans_freq_mon_rec = KMeans(n_clusters=2)\n",
        "kmeans_freq_mon_rec.fit(X)\n",
        "y_kmeans= kmeans_freq_mon_rec.predict(X)"
      ],
      "metadata": {
        "id": "g0mXXzBZmhfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the clusters for the observation given in the dataset\n",
        "rfm_df['Cluster_based_on_freq_mon_rec'] = kmeans_freq_mon_rec.labels_\n",
        "rfm_df.head(10)"
      ],
      "metadata": {
        "id": "ZvmbTHG_ml2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Centers of the clusters(coordinates)\n",
        "centers = kmeans_freq_mon_rec.cluster_centers_\n",
        "centers"
      ],
      "metadata": {
        "id": "ZwTzu4tkmyUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting visualizing the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('customer segmentation based on Recency,Frequency and Monetary')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='winter')\n",
        "\n",
        "centers = kmeans_freq_mon_rec.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8)"
      ],
      "metadata": {
        "id": "RTeV1k7Sm3UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ML Model - 2**\n",
        "\n",
        "#**HIERARCHICAL CLUSTERING**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters.\n",
        "\n",
        "###Strategies for hierarchical clustering generally fall into two categories:\n",
        "\n",
        "###**Agglomerative:** This is a \"bottom-up\" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
        "\n",
        "###**Divisive:** This is a \"top-down\" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
        "\n",
        "###In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering[1] are usually presented in a **dendrogram** (A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.\n",
        "\n",
        "###We can set a threshold distance and draw a horizontal line (Generally, we try to set the threshold in such a way that it cuts the tallest vertical line. Find largest vertical distance we can make without crossing any other horizontal line)."
      ],
      "metadata": {
        "id": "YjMvA2NFuMVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch"
      ],
      "metadata": {
        "id": "NyF0Fs9GvG0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=80, color='r', linestyle='--')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "lJDKJ1FpvLVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The number of clusters will be the number of vertical lines which are being intersected by the line drawn using the threshold.**\n",
        "\n",
        "##**No. of Cluster = 2**"
      ],
      "metadata": {
        "id": "iR_aPFr9vmUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering on  dataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "h_clustering = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = h_clustering.fit_predict(X)"
      ],
      "metadata": {
        "id": "fQil_KDSvw40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Customer 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Customer 2')\n",
        "#plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')\n",
        "\n",
        "plt.title('Clusters of Customer')\n",
        "plt.xlabel('RFM')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TG8i89obv3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_df.head()"
      ],
      "metadata": {
        "id": "TiSaHca0v_kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Firstly I did clustering based on RFM analysis. We had 4 clusters/Segmentation of customers based on RFM score which are as follows:**\n"
      ],
      "metadata": {
        "id": "yC1AVZdvxCVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_based_on_RFM = rfm_df[['Recency','Frequency','Monetary','RFM_Loyalty_Level']]\n",
        "\n",
        "segmentation_based_on_RFM.groupby('RFM_Loyalty_Level').agg({\n",
        "    'Recency': ['mean', 'min', 'max'],\n",
        "    'Frequency': ['mean', 'min', 'max'],\n",
        "    'Monetary': ['mean', 'min', 'max','count']\n",
        "})"
      ],
      "metadata": {
        "id": "B3OlbfScpRRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. Later implemented the machine learning algorithms to cluster the customers.**"
      ],
      "metadata": {
        "id": "PzbGKff7xQAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_process_normalized=rfm_df[['Recency','Frequency','Monetary','Recency_log','Frequency_log','Monetary_log','RFM_Loyalty_Level','Cluster_based_on_freq_mon_rec']]\n",
        "data_process_normalized.groupby('Cluster_based_on_freq_mon_rec').agg({\n",
        "    'Recency': ['mean', 'min', 'max'],\n",
        "    'Frequency': ['mean', 'min', 'max'],\n",
        "    'Monetary': ['mean', 'min', 'max','count']\n",
        "})"
      ],
      "metadata": {
        "id": "Sx7NcfFmn3WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Above clustering is done with recency,frequency and monetary data(Kmeans Clustering) as all 3 together will provide more information.**\n",
        "\n",
        "###**Cluster 0 has high recency rate but very low frequency and monetary. Cluster 0 conatins 2414 customers.**\n",
        "\n",
        "###**Cluster 1 has low recency rate but they are frequent buyers and spends very high money than other customers as mean monetary value is very high.Thus generates more revnue to the retail business.**\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}